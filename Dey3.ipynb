{
  "cells": [
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "vjoXjjOj2Kaq"
      },
      "source": [
        "# Import Libraries/Packages & Datasets"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "eMvQwUpC2CyT"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import seaborn as sns\n",
        "import matplotlib.pyplot as plt\n",
        "import random\n",
        "from tqdm import tqdm\n",
        "import time\n",
        "import copy\n",
        "\n",
        "from sklearn import preprocessing\n",
        "from sklearn.preprocessing import MinMaxScaler\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.utils import shuffle\n",
        "\n",
        "import torch\n",
        "from torch import nn, optim\n",
        "import torch.nn.functional as F\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "\n",
        "torch.backends.cudnn.benchmark=True\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "torch.cuda.manual_seed(5703)\n",
        "torch.manual_seed(5703)\n",
        "np.random.seed(5703)\n",
        "random.seed(5703)"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Set Working directory\n",
        "## Load benign data, drop unwanted columns\n",
        " "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "Label\n",
              "Benign                      4836398\n",
              "DDoS attacks-LOIC-HTTP       573347\n",
              "DoS attacks-Hulk             439126\n",
              "DDOS attack-HOIC             360833\n",
              "Bot                          285763\n",
              "FTP-BruteForce               193354\n",
              "SSH-Bruteforce               187589\n",
              "Infilteration                152874\n",
              "DoS attacks-SlowHTTPTest     139890\n",
              "DoS attacks-GoldenEye         39924\n",
              "DoS attacks-Slowloris          2724\n",
              "DDOS attack-LOIC-UDP           1730\n",
              "Brute Force -Web & XSS          544\n",
              "Name: count, dtype: int64"
            ]
          },
          "execution_count": 2,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "fulldata = pd.read_csv('idsnew\\CICIDS_ALLATTACKS.csv')\n",
        "fulldata['Label'].value_counts()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Benign\n",
            "Bot\n",
            "DDOS attack-HOIC\n",
            "DDoS attacks-LOIC-HTTP\n",
            "DoS attacks-Hulk\n",
            "DoS attacks-SlowHTTPTest\n",
            "FTP-BruteForce\n",
            "Infilteration\n",
            "SSH-Bruteforce\n"
          ]
        }
      ],
      "source": [
        "train_datas = {}\n",
        "test_datas = {}\n",
        "mixed_data = pd.DataFrame()\n",
        "sel_labels = []\n",
        "for i in fulldata['Label'].unique():\n",
        "    if fulldata[fulldata['Label'] == i].shape[0] > 100000:\n",
        "        print(i)\n",
        "        sel_labels.append(i)\n",
        "        train_datas[i] = fulldata[fulldata['Label'] == i][:100000]\n",
        "        test_datas[i] = fulldata[fulldata['Label'] == i][100000:110000]\n",
        "        mixed_data = pd.concat([mixed_data, fulldata[fulldata['Label'] == i][110000:120000]], axis=0)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[[1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0], [0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0], [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0], [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0], [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0]]\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "{'Benign': [1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0],\n",
              " 'Bot': [0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0],\n",
              " 'DDOS attack-HOIC': [0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0],\n",
              " 'DDoS attacks-LOIC-HTTP': [0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0],\n",
              " 'DoS attacks-Hulk': [0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0],\n",
              " 'DoS attacks-SlowHTTPTest': [0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0],\n",
              " 'FTP-BruteForce': [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0],\n",
              " 'Infilteration': [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0],\n",
              " 'SSH-Bruteforce': [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0]}"
            ]
          },
          "execution_count": 4,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "from sklearn.preprocessing import OneHotEncoder\n",
        "labelencoder = OneHotEncoder()\n",
        "labels = np.array(sel_labels).reshape(-1,1)\n",
        "labelencoder.fit(labels)\n",
        "encodedlabels = (labelencoder.transform(labels).todense()).tolist()\n",
        "print(encodedlabels)\n",
        "dikt = {}\n",
        "for i in range(len(encodedlabels)):\n",
        "    dikt[labels[i][0]] = encodedlabels[i]\n",
        "dikt"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "\"for i,row in soft_data.iterrows():\\n    soft_data.at[i,'Label'] = dikt[soft_data.at[i,'Label']]\""
            ]
          },
          "execution_count": 5,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "\"\"\"for i,row in soft_data.iterrows():\n",
        "    soft_data.at[i,'Label'] = dikt[soft_data.at[i,'Label']]\"\"\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {},
      "outputs": [],
      "source": [
        "del fulldata"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {},
      "outputs": [],
      "source": [
        "mixed_data = mixed_data.sample(frac=1).reset_index(drop=True)\n",
        "mixed_data_labels = list(mixed_data['Label'])\n",
        "mixed_data.drop(['Label'], axis=1, inplace=True)\n",
        "mixed_data = mixed_data.to_numpy()\n",
        "for i in train_datas:\n",
        "    train_datas[i].drop(['Label'], axis=1, inplace=True)\n",
        "    test_datas[i].drop(['Label'], axis=1, inplace=True)\n",
        "    train_datas[i] = train_datas[i].sample(frac=1).reset_index(drop=True)\n",
        "    test_datas[i] = test_datas[i].sample(frac=1).reset_index(drop=True)\n",
        "    train_datas[i] = train_datas[i].to_numpy()\n",
        "    test_datas[i] = test_datas[i].to_numpy()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {},
      "outputs": [],
      "source": [
        "mixed_data_labels_encoded = []\n",
        "for i in mixed_data_labels:\n",
        "    mixed_data_labels_encoded.append(dikt[i])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "id": "LCBO3J8zgdgJ"
      },
      "outputs": [],
      "source": [
        "#loading data into pytorch dataloader as train and test\n",
        "devices = []\n",
        "batch_size = 128\n",
        "train_loader = {}\n",
        "for i in train_datas:\n",
        "  train_loader[i] = torch.utils.data.DataLoader(train_datas[i], batch_size = batch_size, shuffle=True)\n",
        "  devices.append(i)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "9"
            ]
          },
          "execution_count": 10,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "len(train_loader)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "id": "R7qV9yUoWfUg",
        "outputId": "85e3c27b-8c1c-4034-f7c8-d9877b4beb87"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "9"
            ]
          },
          "execution_count": 11,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "len(devices)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "id": "XAsJUgfC5IuY"
      },
      "outputs": [],
      "source": [
        "#initialising config variables\n",
        "num_clients = len(train_loader)      # Number of clients\n",
        "num_selected = len(train_loader)    # Typically, num_selected is around 30â€“40% of the num_clients.\n",
        "#baseline_num = 1000  # choose some data from the train set to retrain the data from trained model\n",
        "num_rounds = 5  #100      # Total number of communication rounds for the global model to train.\n",
        "epochs = 50          # for train client model\n",
        "#retrain_epochs = 3  # Total number of retraining rounds on the global server after receiving the model weights\n",
        "                      # from all the clients that participated in the communication round."
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "uF4N6pX57Bmo"
      },
      "source": [
        "## FedAvg / FedAvgM"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "id": "GEhJPuT5633Z"
      },
      "outputs": [],
      "source": [
        "# aggregates the model weights received from every client\n",
        "# and updates the global model with updated weights\n",
        "\n",
        "# FedAvg\n",
        "def server_aggregate(global_model, client_models, client_lens):\n",
        "    total = sum(client_lens)\n",
        "    n = len(client_models)\n",
        "    # n = num_selected\n",
        "    global_dict = global_model.state_dict()\n",
        "    for k in global_dict.keys(): # calculate average weight/bias --> avg_w/b\n",
        "        global_dict[k] -= torch.stack([client_models[i].state_dict()[k].float() * (n * client_lens[i] / total) for i in range(len(client_models))], 0).mean(0)\n",
        "    global_model.load_state_dict(global_dict)\n",
        "    #for model in client_models:\n",
        "     #   model.load_state_dict(global_model.state_dict()) # local model get updated weight/bias\n",
        "\n",
        "# FedAvgM\n",
        "def server_aggregate_M(global_model, client_models, client_lens):\n",
        "    total = sum(client_lens)    # 592    sum [51, 122, 162, 257]\n",
        "    n = len(client_models)      # 4 local clients\n",
        "    global_dict = global_model.state_dict() # weight/bias dict --> {'encoder.0.weight': Tensor with shape torch.Size([86, 115]), 'encoder.0.bias':....} 16 items\n",
        "    temp = copy.deepcopy(global_dict)       # temporary weight/bias dict\n",
        "    v = {x:1 for x in copy.deepcopy(global_dict)}   # initialise v\n",
        "\n",
        "    for i,k in enumerate(global_dict.keys()):\n",
        "        # calculate average weight/bias --> avg_w/b\n",
        "        temp[k] = torch.stack([client_models[i].state_dict()[k].float() * (n * client_lens[i] / total) for i in range(len(client_models))], 0).mean(0)\n",
        "        temp_v = 0.9 * v[k] + temp[k]               # v = 0.9v + avg_w/b   momentum=0.9\n",
        "        global_dict[k] = global_dict[k] - temp_v    # w = w - v\n",
        "    global_model.load_state_dict(global_dict)\n",
        "\n",
        "    #for model in client_models:\n",
        "     #   model.load_state_dict(global_model.state_dict()) # local model get updated weight/bias\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "id": "iqaFf_cS5Ifg"
      },
      "outputs": [],
      "source": [
        "# for updates/trains client model on client data\n",
        "# local training round that takes place for every selected client\n",
        "\n",
        "def client_update(client_model, optimizer, train_data, epoch=3):\n",
        "    client_model.train()\n",
        "    for e in range(epoch):\n",
        "        running_loss = 0.0\n",
        "        for data in train_data:\n",
        "          output = client_model(data.float()) # tensor 115\n",
        "          optimizer.zero_grad()\n",
        "          # criterion = nn.MSELoss(reduction='mean')\n",
        "          # loss = criterion(data.to(device), output)\n",
        "          loss = nn.MSELoss(reduction='mean')(data.float().to(device), output)\n",
        "          loss.backward()\n",
        "          optimizer.step()\n",
        "          # print(loss.item())\n",
        "          running_loss += loss.item()\n",
        "        # print(running_loss)\n",
        "        epoch_loss = running_loss/len(train_data)\n",
        "        # metrics['train_loss'].append(epoch_loss)\n",
        "    return epoch_loss\n",
        "    # return loss.item()\n",
        "\n",
        "\n",
        "\n",
        "# synchronizes the client model with global weights (before training)\n",
        "\n",
        "def client_syn(client_model, global_model):\n",
        "    client_model.load_state_dict(global_model.state_dict())"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "sPE5Fx5I7HZM"
      },
      "source": [
        "## Deep Auto-encoder Model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {
        "id": "QkVroa1V5IjN"
      },
      "outputs": [],
      "source": [
        "import torch.nn as nn\n",
        "\n",
        "input_dim = train_loader['Benign'].dataset.shape[1]\n",
        "\n",
        "class AEModel(nn.Module):\n",
        "    def __init__(self, input_dim):\n",
        "        super(AEModel, self).__init__()\n",
        "        \n",
        "        xavier_gain = nn.init.calculate_gain('tanh')\n",
        "        \n",
        "        self.fc1 = nn.Linear(input_dim, input_dim)\n",
        "        self.fc2 = nn.Linear(input_dim, 32)  \n",
        "        self.fc3 = nn.Linear(32, 16)\n",
        "        self.fc4 = nn.Linear(16, 8)\n",
        "        self.fc5 = nn.Linear(8, 8)\n",
        "        self.fc6 = nn.Linear(8, 16)\n",
        "        self.fc7 = nn.Linear(16, 32)\n",
        "        self.fc8 = nn.Linear(32, input_dim)\n",
        "        self.fc9 = nn.Linear(input_dim, input_dim)\n",
        "       \n",
        "        self.activation = nn.Tanh()\n",
        "        \n",
        "        nn.init.xavier_uniform_(self.fc1.weight, gain=xavier_gain)\n",
        "        nn.init.xavier_uniform_(self.fc2.weight, gain=xavier_gain)\n",
        "        nn.init.xavier_uniform_(self.fc3.weight, gain=xavier_gain) \n",
        "        nn.init.xavier_uniform_(self.fc4.weight, gain=xavier_gain)\n",
        "        nn.init.xavier_uniform_(self.fc5.weight, gain=xavier_gain)\n",
        "        nn.init.xavier_uniform_(self.fc6.weight, gain=xavier_gain)\n",
        "        nn.init.xavier_uniform_(self.fc7.weight, gain=xavier_gain)\n",
        "        nn.init.xavier_uniform_(self.fc8.weight, gain=xavier_gain)\n",
        "        nn.init.xavier_uniform_(self.fc9.weight, gain=xavier_gain)\n",
        "        \n",
        "    def forward(self, x):\n",
        "        x = self.fc1(x) \n",
        "        x = self.activation(x)\n",
        "        x = self.fc2(x)\n",
        "        x = self.activation(x)\n",
        "        x = self.fc3(x)\n",
        "        x = self.activation(x)\n",
        "        x = self.fc4(x) \n",
        "        x = self.activation(x)\n",
        "        x = self.fc5(x)\n",
        "        x = self.activation(x) \n",
        "        x = self.fc6(x)\n",
        "        x = self.activation(x)\n",
        "        x = self.fc7(x)\n",
        "        x = self.activation(x)\n",
        "        x = self.fc8(x) \n",
        "        x = self.activation(x)\n",
        "        x = self.fc9(x)\n",
        "        return x"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "aAX3Z0BI8JDQ"
      },
      "source": [
        "## Initialisation"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {
        "id": "ZDbfUGGW63zk"
      },
      "outputs": [],
      "source": [
        "# Initializing models and optimizer\n",
        "\n",
        "global_model = AEModel(input_dim).to(device)\n",
        "client_models = [AEModel(input_dim).to(device) for _ in range(num_selected)] # part or all clients\n",
        "# print(client_models)\n",
        "for model in client_models:\n",
        "    model.load_state_dict(global_model.state_dict())\n",
        "\n",
        "# method_env = {'lr': (0.012, 0.012, 0.012, 0.0005)}\n",
        "#opt = [torch.optim.SGD(model.parameters(), lr = 0.012, weight_decay=1e-05, momentum=0.9) for model in client_models]\n",
        "# lr_list = [0.012, 0.012, 0.012, 0.00005]\n",
        "# # opt = [torch.optim.SGD(model.parameters(), lr = lr_list[client_models.index(model)], weight_decay=1e-05, momentum=0.9) for model in client_models]\n",
        "# opt = [torch.optim.Adam(model.parameters(), lr = lr_list[client_models.index(model)], weight_decay=1e-05) for model in client_models]\n",
        "opt = [torch.optim.Adam(model.parameters(), lr = 0.012, weight_decay=1e-05) for model in client_models]"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "YuOW_VP89i7j"
      },
      "source": [
        "# Training Federated Model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "rT-jMXex63t8",
        "outputId": "dcce896e-cf42-49da-b608-95868864b0a7"
      },
      "outputs": [],
      "source": [
        "# Train Client Model and Global Model\n",
        "train_loss_client = []\n",
        "train_loss_global = []\n",
        "train_loss_per_client = [[] for i in range(num_clients)]\n",
        "train_loss_benign = []\n",
        "\n",
        "start_time = time.time()\n",
        "\n",
        "for r in range(num_rounds): # total number of rounds\n",
        "\n",
        "    print('\\nround: ', r+1)\n",
        "    # num_selected = random.choice(range(1,num_clients))\n",
        "    #client_idx = np.random.permutation(num_clients)[:num_selected] # random pick some clients from all clients for train\n",
        "    client_idx = list(range(num_clients))\n",
        "    print('client_idx: ', client_idx)\n",
        "    client_lens = [len(train_loader[devices[idx]]) for idx in client_idx ] # Need to fix this. It is part of server aggregation.\n",
        "    # client_lens = [len(train_lotrain_loaderader[idx]) for idx in client_idx ]\n",
        "    #print(client_lens)\n",
        "\n",
        "\n",
        "    #### client update ####\n",
        "    loss = 0\n",
        "    for i in tqdm(range(num_selected)):\n",
        "        # print(i)\n",
        "        #client_syn(client_models[i], global_model)\n",
        "        # print(train_loader[devices[client_idx[i]]])\n",
        "        if devices[client_idx[i]] != 'Benign':\n",
        "            l = client_update(client_models[i], opt[i], train_loader[devices[client_idx[i]]], epochs)\n",
        "            train_loss_per_client[i].append(l)\n",
        "            loss += l\n",
        "    train_loss_client.append(loss)\n",
        "\n",
        "    #### benign client update ####\n",
        "    for i in tqdm(range(1)):\n",
        "        l = client_update(client_models[-1], opt[-1], train_loader['Benign'], epochs)\n",
        "        train_loss_benign.append(l)\n",
        "    train_loss_per_client[-1] = train_loss_benign\n",
        "    train_loss_client.append(sum(train_loss_benign))\n",
        "\n",
        "\n",
        "    #### retraining on the global server ####\n",
        "    #loss_retrain = 0\n",
        "    #for i in tqdm(range(num_selected)):\n",
        "    #    loss_retrain += client_update(client_models[i], opt[i], baseline_data[devices[client_idx[i]]], retrain_epochs)\n",
        "    #train_loss_global.append(loss_retrain/num_selected)\n",
        "\n",
        "    attackclients = []\n",
        "    for i in devices:\n",
        "        if i != 'Benign':\n",
        "            attackclients.append(client_models[devices.index(i)])\n",
        "    \n",
        "    ### aggregate models ###\n",
        "    server_aggregate(global_model, attackclients, client_lens)      # FedAvg\n",
        "    #server_aggregate_M(global_model, attackclients, client_lens)    # FedAvgM\n",
        "\n",
        "    print(\"\\nclient_loss: \", loss)\n",
        "    #print('global_loss: ', loss_retrain/num_selected)\n",
        "\n",
        "time_required = time.time() - start_time\n",
        "print('/nTIME: {}mins'.format(time_required/60))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "for i in range(len(train_loss_per_client)):\n",
        "  print(train_loss_per_client[i])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "mTE7-ISNDA9e"
      },
      "outputs": [],
      "source": [
        "# save model\n",
        "torch.save(global_model,'C:/Final Year Project/modelsnew/globalattackmodel.pt')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "WbFb24eT6ggV"
      },
      "outputs": [],
      "source": [
        "for i in range(len(devices)):\n",
        "  torch.save(client_models[i],'C:/Final Year Project/modelsnew/clients/' + devices[i] + '.pt')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "oI1CQSuoDA9f"
      },
      "outputs": [],
      "source": [
        "losspic = 1\n",
        "def printperformance(losses, modelname):\n",
        "    global losspic\n",
        "    plt.plot(losses)\n",
        "    plt.xlabel('number of rounds')\n",
        "    plt.ylabel('loss')\n",
        "    plt.ylim((0, 2.5))\n",
        "    plt.grid()\n",
        "    plt.title(modelname)\n",
        "    plt.show()\n",
        "    plt.savefig(\"C:/Final Year Project/images/\" + str(modelname) + str(losspic) + \".png\")\n",
        "    losspic += 1"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Sb5v5VOjDA9f",
        "outputId": "cf345ffd-04e8-4892-f7be-a12d48d6dc5f"
      },
      "outputs": [],
      "source": [
        "for i in range(len(devices)-1):\n",
        "  printperformance(train_loss_per_client[i], devices[i])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# print performance of benign client\n",
        "printperformance(train_loss_benign, devices[-1])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Ix3zjB1U63cb",
        "outputId": "b1c7278e-2873-46f8-c710-5ae420f4cf5a"
      },
      "outputs": [],
      "source": [
        "# train_loss_global\n",
        "plt.plot(train_loss_global)\n",
        "plt.xlabel('number of rounds')\n",
        "plt.ylabel('loss')\n",
        "plt.ylim((0, 2.5))\n",
        "plt.grid()\n",
        "plt.title('Federated Model with FedAvg (use five devices to train)')\n",
        "plt.show()\n",
        "plt.savefig(\"C:/Final Year Project/images/Global10.png\")"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "dK43oLbY9uNr"
      },
      "source": [
        "# Evaluation"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {
        "id": "ku4dyUThDZF1"
      },
      "outputs": [],
      "source": [
        "client_models = {}\n",
        "for device in devices:\n",
        "  client_models[device] = torch.load('C:/Final Year Project/modelsnew/clients/' + device + '.pt')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {},
      "outputs": [],
      "source": [
        "def forward(self, x):\n",
        "        x = self.fc1(x) \n",
        "        x = self.activation(x)\n",
        "        x = self.fc2(x)\n",
        "        x = self.activation(x)\n",
        "        x = self.fc3(x)\n",
        "        x = self.activation(x)\n",
        "        x = self.fc4(x) \n",
        "        x = self.activation(x)\n",
        "        x = self.fc5(x)\n",
        "        x = self.activation(x) \n",
        "        x = self.fc6(x)\n",
        "        x = self.activation(x)\n",
        "        x = self.fc7(x)\n",
        "        x = self.activation(x)\n",
        "        x = self.fc8(x) \n",
        "        x = self.activation(x)\n",
        "        x = self.fc9(x)\n",
        "        x = self.activation(x)\n",
        "        x = self.fc10(x)\n",
        "        x = self.Softmax(x)\n",
        "        return x\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "metadata": {},
      "outputs": [],
      "source": [
        "for i in client_models:\n",
        "    client_models[i].add_module('fc10', nn.Linear(input_dim, len(devices)))\n",
        "    client_models[i].add_module('Softmax', nn.Softmax(dim=1))\n",
        "    client_models[i].forward = forward"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "AEModel(\n",
              "  (fc1): Linear(in_features=69, out_features=69, bias=True)\n",
              "  (fc2): Linear(in_features=69, out_features=32, bias=True)\n",
              "  (fc3): Linear(in_features=32, out_features=16, bias=True)\n",
              "  (fc4): Linear(in_features=16, out_features=8, bias=True)\n",
              "  (fc5): Linear(in_features=8, out_features=8, bias=True)\n",
              "  (fc6): Linear(in_features=8, out_features=16, bias=True)\n",
              "  (fc7): Linear(in_features=16, out_features=32, bias=True)\n",
              "  (fc8): Linear(in_features=32, out_features=69, bias=True)\n",
              "  (fc9): Linear(in_features=69, out_features=69, bias=True)\n",
              "  (activation): Tanh()\n",
              "  (fc10): Linear(in_features=69, out_features=9, bias=True)\n",
              "  (Softmax): Softmax(dim=1)\n",
              ")"
            ]
          },
          "execution_count": 20,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "client_models['Bot']"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def client_update_soft(client_model, optimizer, train_data, target_label, epoch=3):\n",
        "    client_model.train()\n",
        "    for e in range(epoch):\n",
        "        running_loss = 0.0\n",
        "        for batch in train_data:\n",
        "          for data in batch:\n",
        "            output = client_model(data.float()) # tensor 115\n",
        "            optimizer.zero_grad()\n",
        "            # criterion = nn.MSELoss(reduction='mean')\n",
        "            # loss = criterion(data.to(device), output)\n",
        "            loss = nn.CrossEntropyLoss(output, target_label)\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "            # print(loss.item())\n",
        "            running_loss += loss.item()\n",
        "          # print(running_loss)\n",
        "        epoch_loss = running_loss/len(train_data)\n",
        "        # metrics['train_loss'].append(epoch_loss)\n",
        "    return epoch_loss"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Train Client Model and Global Model\n",
        "train_loss_client = []\n",
        "train_loss_global = []\n",
        "train_loss_per_client = [[] for i in range(num_clients)]\n",
        "train_loss_benign = []\n",
        "\n",
        "start_time = time.time()\n",
        "\n",
        "for r in range(num_rounds): # total number of rounds\n",
        "\n",
        "    print('\\nround: ', r+1)\n",
        "    # num_selected = random.choice(range(1,num_clients))\n",
        "    #client_idx = np.random.permutation(num_clients)[:num_selected] # random pick some clients from all clients for train\n",
        "    client_idx = list(range(num_clients))\n",
        "    print('client_idx: ', client_idx)\n",
        "    client_lens = [len(train_loader[devices[idx]]) for idx in client_idx ] # Need to fix this. It is part of server aggregation.\n",
        "    # client_lens = [len(train_lotrain_loaderader[idx]) for idx in client_idx ]\n",
        "    #print(client_lens)\n",
        "\n",
        "\n",
        "    #### client update ####\n",
        "    loss = 0\n",
        "    for i in tqdm(range(num_selected)):\n",
        "        # print(i)\n",
        "        #client_syn(client_models[i], global_model)\n",
        "        # print(train_loader[devices[client_idx[i]]])\n",
        "        if devices[client_idx[i]] != 'Benign':\n",
        "            model = client_models[devices[client_idx[i]]]\n",
        "            l = client_update(client_models[devices[client_idx[i]]], opt[i], train_loader[devices[client_idx[i]]], dikt[devices[client_idx[i]]], epochs)\n",
        "            train_loss_per_client[i].append(l)\n",
        "            loss += l\n",
        "    train_loss_client.append(loss)\n",
        "\n",
        "    #### benign client update ####\n",
        "    for i in tqdm(range(1)):\n",
        "        l = client_update(client_models['Benign'], opt[-1], train_loader['Benign'], dikt['Benign'], epochs)\n",
        "        train_loss_benign.append(l)\n",
        "    train_loss_per_client[-1] = train_loss_benign\n",
        "    train_loss_client.append(sum(train_loss_benign))\n",
        "\n",
        "\n",
        "    #### retraining on the global server ####\n",
        "    #loss_retrain = 0\n",
        "    #for i in tqdm(range(num_selected)):\n",
        "    #    loss_retrain += client_update(client_models[i], opt[i], baseline_data[devices[client_idx[i]]], retrain_epochs)\n",
        "    #train_loss_global.append(loss_retrain/num_selected)\n",
        "\n",
        "    attackclients = []\n",
        "    for i in devices:\n",
        "        if i != 'Benign':\n",
        "            attackclients.append(client_models[i])\n",
        "    \n",
        "    ### aggregate models ###\n",
        "    server_aggregate(global_model, attackclients, client_lens)      # FedAvg\n",
        "    #server_aggregate_M(global_model, attackclients, client_lens)    # FedAvgM\n",
        "\n",
        "    print(\"\\nclient_loss: \", loss)\n",
        "    #print('global_loss: ', loss_retrain/num_selected)\n",
        "\n",
        "time_required = time.time() - start_time\n",
        "print('/nTIME: {}mins'.format(time_required/60))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1DvI0napggwm"
      },
      "outputs": [],
      "source": [
        "from sklearn import metrics"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "h1HJ9RZw63Z-"
      },
      "outputs": [],
      "source": [
        "def get_thresh(model, dataloader):\n",
        "\n",
        "    model.eval()\n",
        "    errs = []\n",
        "    mses = []\n",
        "    for batch in dataloader:\n",
        "       for data in batch:\n",
        "         mse = np.power(data.float().cpu().detach().numpy() - model(data.float()).cpu().detach().numpy(), 2) #before np.mean(..., axis=0) (len(mse) = 56)\n",
        "         mses.append(mse)\n",
        "    mses = np.array(mses)\n",
        "    print(\"MSES: \" + str(mses.shape))\n",
        "    thresh = mses.mean(axis=0) + mses.std(axis=0)\n",
        "    print(thresh.shape)\n",
        "    #errs.append(thresh)\n",
        "    #thresh = sum(errs) / len(errs)\n",
        "    return thresh\n",
        "\n",
        "    #model.eval()\n",
        "    #pred = global_model(tr_data).cpu().detach().numpy()\n",
        "    #print(pred)\n",
        "    #print(np.isnan(pred))\n",
        "    #thresh = np.sqrt(metrics.mean_squared_error(pred, threshben.numpy()))\n",
        "    #mse = np.mean(np.power(tr_data.cpu().detach().numpy() - model(tr_data).cpu().detach().numpy(), 2), axis=1)\n",
        "    #tr = np.sqrt(metrics.mean_squared_error(model(tr_data).cpu().detach().numpy(), tr_data.cpu().detach().numpy()))\n",
        "    #tr = np.sqrt(mse)\n",
        "    #return thresh\n",
        "\n",
        "    \n",
        "def quan_thresh(model, dataloader, quantile=0.9):\n",
        "\n",
        "  model.eval()\n",
        "  errs = []\n",
        "  se = []\n",
        "  \n",
        "  for batch in dataloader:\n",
        "    \n",
        "    for data in batch:\n",
        "      error = np.power(data.float().cpu().numpy() - model(data.float()).cpu().detach().numpy(), 2) # len(error) = 66\n",
        "      se.append(sum(error))\n",
        "    # Calculate threshold as quantile of errors \n",
        "    #thresh = np.quantile(se, quantile, axis=0) #before it was thresh = np.quantile(mse, quantile)\n",
        "    #errs.append(thresh)\n",
        "  #thresh = np.quantile(mse, quantile)\n",
        "  return np.mean(se) + np.std(se), np.quantile(se, quantile), se #before it was np.mean(errs)\n",
        "  #return thresh\n",
        "\n",
        "def quan_recon(model, dataloader, quantile=0.8):\n",
        "  model.eval()\n",
        "  datas = []\n",
        "  for batch in dataloader:\n",
        "    for data in batch:\n",
        "      datas.append(model(data.float()).cpu().detach().numpy())\n",
        "  thresh = np.quantile(datas, quantile, axis=0)\n",
        "  return thresh\n",
        "\n",
        "def perf_measure(y_actual, y_pred):\n",
        "    TP = 0\n",
        "    FP = 0\n",
        "    TN = 0\n",
        "    FN = 0\n",
        "    for i in range(len(y_pred)):\n",
        "        if y_actual[i] == y_pred[i] == 1:\n",
        "           TP += 1\n",
        "        if y_pred[i] == 1 and y_actual[i] != y_pred[i]:\n",
        "           FP += 1\n",
        "        if y_actual[i] == y_pred[i] == 0:\n",
        "           TN += 1\n",
        "        if y_pred[i] == 0 and y_actual[i] != y_pred[i]:\n",
        "           FN += 1\n",
        "    return (TP, FP, TN, FN)\n",
        "\n",
        "def get_mix_result(model, tr, mix_data, mix_label):\n",
        "    model.eval()\n",
        "    prediction = []\n",
        "    for i in mix_data:\n",
        "      pred = model(i).cpu().detach().numpy()\n",
        "      error = np.sqrt(metrics.mean_squared_error(pred, threshben.numpy()))\n",
        "      if error > tr:\n",
        "        prediction.append(1)\n",
        "      else:\n",
        "        prediction.append(0)\n",
        "\n",
        "    mix_label_list = mix_label.tolist()\n",
        "\n",
        "    TP,FP,TN,FN = perf_measure(mix_label_list, prediction)\n",
        "\n",
        "    conf = [[TP, FN],[FP, TN]]\n",
        "    print(conf)\n",
        "    x_axis_label = ['abnormal', 'benign']\n",
        "    y_axis_label = ['abnormal', 'benign']\n",
        "\n",
        "    plt.figure()\n",
        "    sns.heatmap(conf,xticklabels=x_axis_label, yticklabels=y_axis_label, annot=True,annot_kws={\"size\": 16},fmt='g')\n",
        "\n",
        "    acc = (TP+TN) / (TP+TN+FP+FN)\n",
        "    precision = TP/(TP+FP)\n",
        "    recall = TP/(TP+FN)\n",
        "    F1score = 2 * ((precision * recall) / (precision + recall))\n",
        "\n",
        "    TPR = round((TP / (TP+FN)), 6)\n",
        "    # print('TPR is: {}%'.format(TPR))\n",
        "\n",
        "    FPR = round((FP / (FP + TN)), 6)\n",
        "    # print('TPR is: {}%'.format(FPR))\n",
        "\n",
        "    print('Acc: %.3f%% \\nPrecision: %.3f \\nRecall: %.3f \\nF1score: %.3f \\nTPR: %.5f \\nFPR: %.5f'%(acc*100,\n",
        "                                                                                                   precision,\n",
        "                                                                                                   recall,\n",
        "                                                                                                   F1score*100,\n",
        "                                                                                                   TPR,\n",
        "                                                                                                   FPR))\n",
        "\n",
        "#   [['TP', 'FN']\n",
        "#   ['FP', 'TN']]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "FhTUbMWYDca5"
      },
      "outputs": [],
      "source": [
        "thresholds = {}\n",
        "quans = {}\n",
        "errors = {}\n",
        "for device in devices:\n",
        "    model = client_models[device]\n",
        "    dataloader = train_loader[device]\n",
        "    thresholds[device], quans[device], errors[device] = quan_thresh(model, dataloader)\n",
        "    #thresholds[devices[i]] = quan_recon(model, dataloader)\n",
        "print(thresholds)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "quans"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "#@thresholds = {'Benign': 61.877983871320254, 'Bot': 0.09083680293611163, 'DDOS attack-HOIC': 1.8556977635303178, 'DoS attacks-Hulk': 0.11400531390823192, 'DoS attacks-SlowHTTPTest': 0.21217828694483437, 'FTP-BruteForce': 0.0029948042571023934, 'Infilteration': 3.799426671646207, 'SSH-Bruteforce': 1.0148292389310203}\n",
        "# mean + std of sse for 2 round 50 epoch"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "#thresholds={'Benign': 61.87798387132028, 'Bot': 0.09161945656969754, 'DDOS attack-HOIC': 0.025858257999281466, 'DoS attacks-Hulk': 0.13915533441081115, 'DoS attacks-SlowHTTPTest': 0.007261200969461876, 'FTP-BruteForce': 0.0034343920695094742, 'Infilteration': 0.832359128599836, 'SSH-Bruteforce': 2.8281726392384123}\n",
        "# 5 round 50 epoch"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "len(thresholds)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "devices"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "c = lambda lst, x: len([y for y in lst if y < x])\n",
        "for i in devices:\n",
        "    print(c(errors[i], quans[i]))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "for i in devices:\n",
        "    print(c(errors[i], thresholds[i]))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def quan_thresh_test(model, testset, quantile=0.9):\n",
        "    model.eval()\n",
        "    se = []\n",
        "    for data in testset:\n",
        "        data = torch.Tensor(data)\n",
        "        error = np.power(data.float().cpu().numpy() - model(data.float()).cpu().detach().numpy(), 2) # len(error) = 66\n",
        "        se.append(sum(error))\n",
        "    return np.mean(se) + np.std(se), np.quantile(se, quantile), se"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "thresholds_test = {}\n",
        "quans_test = {}\n",
        "errors_test = {}\n",
        "for device in devices:\n",
        "    model = client_models[i]\n",
        "    testset = test_datas[device]\n",
        "    thresholds_test[device], quans_test[device], errors_test[device] = quan_thresh_test(model, testset)\n",
        "print(thresholds_test)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "for i in devices:\n",
        "    print(i, c(errors_test[i], thresholds[i]))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "for i in devices:\n",
        "    print(c(errors_test[i], quans[i]))"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### What the model is supposed to find = 1 (Error < Threshold)\n",
        "### What the model is not supposed to find = 0 (Error > Threshold)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "from scipy.stats import spearmanr, rankdata, pearsonr\n",
        "from sklearn.metrics import mutual_info_score\n",
        "selectedattackbenmix_stats = []\n",
        "res=[]\n",
        "device_idx = {device: i for i, device in enumerate(devices)}\n",
        "for device in devices:\n",
        "  model = client_models[device]\n",
        "  model.eval()\n",
        "  labels = device\n",
        "  threshold = thresholds[device]\n",
        "  temp=[]\n",
        "  y_true = []\n",
        "  y_pred = []\n",
        "  ind = 0\n",
        "  for data in test_datas[device]:\n",
        "      data = torch.Tensor(data)\n",
        "      error = np.sum(np.power(data.detach().numpy() - model(data).detach().numpy(), 2))\n",
        "      temp.append(error)\n",
        "      if device == labels:\n",
        "          y_true.append(1)\n",
        "          y_pred.append(1 if error < threshold else 0)\n",
        "      else:\n",
        "          y_true.append(0)\n",
        "          y_pred.append(0 if error >= threshold else 1)\n",
        "      #cf.append([coeff, p, device[:-4] == labels[ind]])\n",
        "      ind += 1\n",
        "  res.append([device,min(temp),max(temp)])\n",
        "  TP, FP, TN, FN = perf_measure(y_true, y_pred)\n",
        "  TP += 1\n",
        "  FN += 1\n",
        "  TN += 1\n",
        "  FP += 1\n",
        "  conf_matrix = [[TP, FN], [FP, TN]]\n",
        "  plt.figure() \n",
        "  sns.heatmap(conf_matrix, annot=True, fmt=\"d\")\n",
        "  plt.title(\"Confusion Matrix for Client \" + str(device) +\" with test data\")\n",
        "  plt.xlabel(\"Predicted Label\")\n",
        "  plt.ylabel(\"True Label\")\n",
        "\n",
        "  acc = (TP+TN) / (TP+TN+FP+FN)\n",
        "  precision = TP/(TP+FP)\n",
        "  recall = TP/(TP+FN)\n",
        "  F1score = 2 * ((precision * recall) / (precision + recall))\n",
        "\n",
        "  TPR = round((TP / (TP+FN)), 6)\n",
        "  # print('TPR is: {}%'.format(TPR))\n",
        "\n",
        "  FPR = round((FP / (FP + TN)), 6)\n",
        "  # print('TPR is: {}%'.format(FPR))\n",
        "\n",
        "  selectedattackbenmix_stats.append([str(\"Stats for Client \" + str(device) +\" with test data\"),acc*100,precision,recall,F1score*100,TPR,FPR])\n",
        "\n",
        "  #   [['TP', 'FN']\n",
        "  #   ['FP', 'TN']]"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Plot for global model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "oomBBYuq63WS"
      },
      "outputs": [],
      "source": [
        "# load model\n",
        "global_model = torch.load('C:/Final Year Project/models/globalattackmodel.pt')                                        "
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "0zKO6BUI97sa"
      },
      "source": [
        "# Testing client models with equal number of all data (selected and not selected)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "from scipy.stats import spearmanr, rankdata, pearsonr\n",
        "from sklearn.metrics import mutual_info_score\n",
        "selectedattackbenmix_stats = []\n",
        "res=[]\n",
        "device_idx = {device: i for i, device in enumerate(devices)}\n",
        "for device in devices:\n",
        "#  if device == 'Infilteration.csv':\n",
        "  model = client_models[device]\n",
        "  model.eval()\n",
        "  labels = mixed_data_labels\n",
        "  threshold = thresholds[device]\n",
        "  temp=[]\n",
        "  y_true = []\n",
        "  y_pred = []\n",
        "  ind = 0\n",
        "  for data in mixed_data:\n",
        "      data = torch.Tensor(data)\n",
        "      error = np.sum(np.power(data.detach().numpy() - model(data).detach().numpy(), 2))\n",
        "      temp.append(error)\n",
        "      if device == labels[ind]:\n",
        "          y_true.append(1)\n",
        "          y_pred.append(1 if error < threshold else 0)\n",
        "      else:\n",
        "          y_true.append(0)\n",
        "          y_pred.append(0 if error >= threshold else 1)\n",
        "      #cf.append([coeff, p, device[:-4] == labels[ind]])\n",
        "      ind += 1\n",
        "  res.append([device,min(temp),max(temp)])\n",
        "  TP, FP, TN, FN = perf_measure(y_true, y_pred)\n",
        "  TP += 1\n",
        "  FN += 1\n",
        "  TN += 1\n",
        "  FP += 1\n",
        "  conf_matrix = [[TP, FN], [FP, TN]]\n",
        "  plt.figure() \n",
        "  sns.heatmap(conf_matrix, annot=True, fmt=\"d\")\n",
        "  plt.title(\"Confusion Matrix for Client \" + str(device) +\" with mixed data\")\n",
        "  plt.xlabel(\"Predicted Label\")\n",
        "  plt.ylabel(\"True Label\")\n",
        "\n",
        "  acc = (TP+TN) / (TP+TN+FP+FN)\n",
        "  precision = TP/(TP+FP)\n",
        "  recall = TP/(TP+FN)\n",
        "  F1score = 2 * ((precision * recall) / (precision + recall))\n",
        "\n",
        "  TPR = round((TP / (TP+FN)), 6)\n",
        "  # print('TPR is: {}%'.format(TPR))\n",
        "\n",
        "  FPR = round((FP / (FP + TN)), 6)\n",
        "  # print('TPR is: {}%'.format(FPR))\n",
        "\n",
        "  selectedattackbenmix_stats.append([str(\"Stats for Client \" + str(device) +\" with mixed data\"),acc*100,precision,recall,F1score*100,TPR,FPR])\n",
        "\n",
        "  #   [['TP', 'FN']\n",
        "  #   ['FP', 'TN']]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "res"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "thresholds"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "for i in selectedattackbenmix_stats:\n",
        "    print(\"Title: \" + i[0])\n",
        "    print(\"Accuracy: \" + str(i[1]))\n",
        "    print(\"Precision: \" + str(i[2]))\n",
        "    print(\"Recall: \" + str(i[3]))\n",
        "    print(\"F1 score : \" + str(i[4]))\n",
        "    print(\"TPR : \" + str(i[5]))\n",
        "    print(\"FPR : \" + str(i[6]))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import matplotlib\n",
        "matplotlib.pyplot.close()"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Testing global model with equal number of all data (selected only) (not selected does not have threshold)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "from scipy.stats import spearmanr, rankdata, pearsonr\n",
        "from sklearn.metrics import mutual_info_score\n",
        "selectedattackbenmix_stats = []\n",
        "res=[]\n",
        "model = global_model\n",
        "model.eval()\n",
        "labels = mixed_data_labels\n",
        "temp=[]\n",
        "y_true = []\n",
        "y_pred = []\n",
        "ind = 0\n",
        "errors_global = []\n",
        "for data in mixed_data:\n",
        "    data = torch.Tensor(data)\n",
        "    error = np.sum(np.power(data.detach().numpy() - model(data).detach().numpy(), 2))\n",
        "    errors_global.append(error)\n",
        "'''\n",
        "    temp.append(error)\n",
        "    threshold = thresholds[labels[ind]]\n",
        "    if labels[ind] == 'Benign':\n",
        "        threshold = max(thresholds.values())\n",
        "        y_true.append(0)\n",
        "        y_pred.append(0 if error >= threshold else 1)\n",
        "    else:\n",
        "        y_true.append(1)\n",
        "        y_pred.append(1 if error <= threshold else 0)\n",
        "    ind += 1\n",
        "res.append([device,min(temp),max(temp)])\n",
        "TP, FP, TN, FN = perf_measure(y_true, y_pred)\n",
        "TP += 1\n",
        "FN += 1\n",
        "TN += 1\n",
        "FP += 1\n",
        "conf_matrix = [[TP, FN], [FP, TN]]\n",
        "plt.figure() \n",
        "sns.heatmap(conf_matrix, annot=True, fmt=\"d\")\n",
        "plt.title(\"Confusion Matrix for Client \" + str(device) +\" with mixed data\")\n",
        "plt.xlabel(\"Predicted Label\")\n",
        "plt.ylabel(\"True Label\")\n",
        "\n",
        "acc = (TP+TN) / (TP+TN+FP+FN)\n",
        "precision = TP/(TP+FP)\n",
        "recall = TP/(TP+FN)\n",
        "F1score = 2 * ((precision * recall) / (precision + recall))\n",
        "\n",
        "TPR = round((TP / (TP+FN)), 6)\n",
        "# print('TPR is: {}%'.format(TPR))\n",
        "\n",
        "FPR = round((FP / (FP + TN)), 6)\n",
        "# print('TPR is: {}%'.format(FPR))\n",
        "\n",
        "selectedattackbenmix_stats.append([str(\"Stats for Client \" + str(device) +\" with mixed data\"),acc*100,precision,recall,F1score*100,TPR,FPR])\n",
        "\n",
        "#   [['TP', 'FN']\n",
        "#   ['FP', 'TN']]\n",
        "'''"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "for i in selectedattackbenmix_stats_global:\n",
        "    print(\"Title: \" + i[0])\n",
        "    print(\"Accuracy: \" + str(i[1]))\n",
        "    print(\"Precision: \" + str(i[2]))\n",
        "    print(\"Recall: \" + str(i[3]))\n",
        "    print(\"F1 score : \" + str(i[4]))\n",
        "    print(\"TPR : \" + str(i[5]))\n",
        "    print(\"FPR : \" + str(i[6]))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import matplotlib\n",
        "matplotlib.pyplot.close()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "print(max(errors_global), min(errors_global))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "thresholds"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "quans"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "mixed_data_labels"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "thresholds"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "clientthresh=[]\n",
        "for i in thresholds:\n",
        "    if i!='Benign':\n",
        "        clientthresh.append(thresholds[i])\n",
        "clientthresh"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "globalmin=min(errors_global)\n",
        "dist=[abs(t-globalmin) for t in clientthresh]\n",
        "weights=[1/d for d in dist]\n",
        "weights=weights/np.sum(weights)\n",
        "weighted_avg_thresh=np.sum(weights*clientthresh)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "dist"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "min(errors_global)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.0"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
